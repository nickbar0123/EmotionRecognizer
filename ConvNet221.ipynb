{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thalvadzhiev_gmail_com/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.layers import ZeroPadding2D\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"faces_dataset_conv.pickle\", \"rb\") as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ds = data[\"train_dataset\"]\n",
    "train_labels = data[\"train_labels\"]\n",
    "pub_test_ds = data[\"public_test\"]\n",
    "pub_test_labels = data[\"public_test_labels\"]\n",
    "priv_test_ds = data[\"private_test\"]\n",
    "priv_test_labels = data[\"private_test_labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, 4, 6, 2, 4, 3, 3, 2], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ds = train_ds.reshape(train_ds.shape[0], 1, 48, 48)\n",
    "pub_test_ds = pub_test_ds.reshape(pub_test_ds.shape[0], 1, 48, 48)\n",
    "priv_test_ds = priv_test_ds.reshape(priv_test_ds.shape[0], 1, 48, 48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_bigger = np.vstack([train_ds, pub_test_ds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32298, 1, 48, 48)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bigger.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels = np_utils.to_categorical(train_labels, 7)\n",
    "pub_test_labels = np_utils.to_categorical(pub_test_labels, 7)\n",
    "priv_test_labels = np_utils.to_categorical(priv_test_labels, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_bigger_labels = np.concatenate([train_labels, pub_test_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ds_inv = train_ds.reshape(train_ds.shape[0], 48, 48, 1)\n",
    "pub_test_ds_inv = pub_test_ds.reshape(pub_test_ds.shape[0], 48, 48, 1)\n",
    "priv_test_ds_inv = priv_test_ds.reshape(priv_test_ds.shape[0],48, 48, 1)\n",
    "train_bigger_inv = np.vstack([train_ds_inv, pub_test_ds_inv])\n",
    "\n",
    "train_labels = np_utils.to_categorical(train_labels, 7)\n",
    "pub_test_labels = np_utils.to_categorical(pub_test_labels, 7)\n",
    "priv_test_labels = np_utils.to_categorical(priv_test_labels, 7)\n",
    "train_bigger_labels = np.concatenate([train_labels, pub_test_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32298, 7)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bigger_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_221_model(weights=None):\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D(padding=(3, 3), input_shape=(1, 48, 48)))\n",
    "    model.add(Convolution2D(filters=128, kernel_size=4))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(ZeroPadding2D(padding=(3, 3)))\n",
    "    model.add(Convolution2D(filters=128, kernel_size=4))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    \n",
    "    model.add(Convolution2D(filters=128, kernel_size=4, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(ZeroPadding2D(padding=(3, 3)))\n",
    "    model.add(Convolution2D(filters=128, kernel_size=4))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    \n",
    "    model.add(Convolution2D(filters=128, kernel_size=4, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(ZeroPadding2D(padding=(3, 3)))\n",
    "    model.add(Convolution2D(filters=128, kernel_size=4))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    \n",
    "    model.add(Convolution2D(filters=128, kernel_size=4, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    ##fully connected\n",
    "    model.add(Dense(units=1024))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(units=1024))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(units=7, activation=\"softmax\"))\n",
    "    \n",
    "    if weights:\n",
    "        model.load_weights(weights)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = get_221_model(\"weights-improvement-04-0.41.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir='./Graph', histogram_freq=1,write_graph=True, write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath = \"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callback_list = [checkpoint, tensorboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25838 samples, validate on 6460 samples\n",
      "Epoch 1/15\n",
      "25838/25838 [==============================] - 50s 2ms/step - loss: 0.9733 - acc: 0.6411 - val_loss: 1.5069 - val_acc: 0.4772\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.49425\n",
      "Epoch 2/15\n",
      "25838/25838 [==============================] - 49s 2ms/step - loss: 0.9072 - acc: 0.6631 - val_loss: 1.4625 - val_acc: 0.4954\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.49425 to 0.49536, saving model to weights-improvement-02-0.50.hdf5\n",
      "Epoch 3/15\n",
      "25838/25838 [==============================] - 49s 2ms/step - loss: 0.8475 - acc: 0.6845 - val_loss: 1.4956 - val_acc: 0.4967\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.49536 to 0.49675, saving model to weights-improvement-03-0.50.hdf5\n",
      "Epoch 4/15\n",
      "25838/25838 [==============================] - 49s 2ms/step - loss: 0.7710 - acc: 0.7145 - val_loss: 1.5504 - val_acc: 0.5012\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.49675 to 0.50124, saving model to weights-improvement-04-0.50.hdf5\n",
      "Epoch 5/15\n",
      "25838/25838 [==============================] - 49s 2ms/step - loss: 0.7141 - acc: 0.7371 - val_loss: 1.6401 - val_acc: 0.5022\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.50124 to 0.50217, saving model to weights-improvement-05-0.50.hdf5\n",
      "Epoch 6/15\n",
      "25838/25838 [==============================] - 49s 2ms/step - loss: 0.6464 - acc: 0.7636 - val_loss: 1.6978 - val_acc: 0.4981\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.50217\n",
      "Epoch 7/15\n",
      "25838/25838 [==============================] - 49s 2ms/step - loss: 0.5814 - acc: 0.7840 - val_loss: 1.7835 - val_acc: 0.5012\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.50217\n",
      "Epoch 8/15\n",
      "25838/25838 [==============================] - 49s 2ms/step - loss: 0.5270 - acc: 0.8087 - val_loss: 1.8043 - val_acc: 0.4910\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.50217\n",
      "Epoch 9/15\n",
      "25838/25838 [==============================] - 49s 2ms/step - loss: 0.4786 - acc: 0.8265 - val_loss: 1.9168 - val_acc: 0.5062\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.50217 to 0.50619, saving model to weights-improvement-09-0.51.hdf5\n",
      "Epoch 10/15\n",
      "25838/25838 [==============================] - 49s 2ms/step - loss: 0.4373 - acc: 0.8405 - val_loss: 1.9956 - val_acc: 0.5012\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.50619\n",
      "Epoch 11/15\n",
      "25838/25838 [==============================] - 49s 2ms/step - loss: 0.3881 - acc: 0.8572 - val_loss: 2.0942 - val_acc: 0.5029\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.50619\n",
      "Epoch 12/15\n",
      "25838/25838 [==============================] - 49s 2ms/step - loss: 0.3680 - acc: 0.8638 - val_loss: 2.1732 - val_acc: 0.5026\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.50619\n",
      "Epoch 13/15\n",
      "25838/25838 [==============================] - 49s 2ms/step - loss: 0.3291 - acc: 0.8830 - val_loss: 2.2262 - val_acc: 0.5036\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.50619\n",
      "Epoch 14/15\n",
      "25838/25838 [==============================] - 49s 2ms/step - loss: 0.3151 - acc: 0.8873 - val_loss: 2.2748 - val_acc: 0.5084\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.50619 to 0.50836, saving model to weights-improvement-14-0.51.hdf5\n",
      "Epoch 15/15\n",
      "25838/25838 [==============================] - 49s 2ms/step - loss: 0.2790 - acc: 0.9007 - val_loss: 2.3201 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.50836\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_bigger, train_bigger_labels, \n",
    "          batch_size=128, validation_split=0.2, epochs=15, verbose=1, callbacks=callback_list)\n",
    "model.save_weights(\"best_weights.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(priv_test_ds, priv_test_labels, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.2812914079728315, 0.5018110894482593]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_221_model2(weights=None):\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D(padding=(3, 3), input_shape=(48, 48, 1)))\n",
    "    model.add(Convolution2D(filters=128, kernel_size=4, data_format=\"channels_last\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(ZeroPadding2D(padding=(3, 3)))\n",
    "    model.add(Convolution2D(filters=128, kernel_size=4, data_format=\"channels_last\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    \n",
    "    model.add(Convolution2D(filters=128, kernel_size=4, padding=\"same\", data_format=\"channels_last\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(ZeroPadding2D(padding=(3, 3)))\n",
    "    model.add(Convolution2D(filters=128, kernel_size=4, data_format=\"channels_last\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    \n",
    "    model.add(Convolution2D(filters=128, kernel_size=4, padding=\"same\", data_format=\"channels_last\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(ZeroPadding2D(padding=(3, 3)))\n",
    "    model.add(Convolution2D(filters=128, kernel_size=4, data_format=\"channels_last\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    \n",
    "    model.add(Convolution2D(filters=128, kernel_size=4, padding=\"same\", data_format=\"channels_last\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    ##fully connected\n",
    "    model.add(Dense(units=1024))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(units=1024))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(units=7, activation=\"softmax\"))\n",
    "    \n",
    "    if weights:\n",
    "        model.load_weights(weights)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2 = get_221_model2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath = \"weights-improvement-model2-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callback_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29068 samples, validate on 3230 samples\n",
      "Epoch 1/10\n",
      "29068/29068 [==============================] - 182s 6ms/step - loss: 0.6444 - acc: 0.7703 - val_loss: 1.1706 - val_acc: 0.5873\n",
      "\n",
      "Epoch 00001: val_acc improved from 0.57554 to 0.58731, saving model to weights-improvement-model2-01-0.59.hdf5\n",
      "Epoch 2/10\n",
      "29068/29068 [==============================] - 182s 6ms/step - loss: 0.5478 - acc: 0.8020 - val_loss: 1.3370 - val_acc: 0.5969\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.58731 to 0.59690, saving model to weights-improvement-model2-02-0.60.hdf5\n",
      "Epoch 3/10\n",
      "29068/29068 [==============================] - 182s 6ms/step - loss: 0.4487 - acc: 0.8355 - val_loss: 1.5784 - val_acc: 0.5604\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.59690\n",
      "Epoch 4/10\n",
      "29068/29068 [==============================] - 182s 6ms/step - loss: 0.3885 - acc: 0.8599 - val_loss: 1.6422 - val_acc: 0.5712\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.59690\n",
      "Epoch 5/10\n",
      "29068/29068 [==============================] - 182s 6ms/step - loss: 0.3199 - acc: 0.8851 - val_loss: 1.6276 - val_acc: 0.5898\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.59690\n",
      "Epoch 6/10\n",
      "29068/29068 [==============================] - 182s 6ms/step - loss: 0.2734 - acc: 0.9020 - val_loss: 1.7026 - val_acc: 0.6115\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.59690 to 0.61146, saving model to weights-improvement-model2-06-0.61.hdf5\n",
      "Epoch 7/10\n",
      "29068/29068 [==============================] - 182s 6ms/step - loss: 0.2294 - acc: 0.9180 - val_loss: 1.8152 - val_acc: 0.6015\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.61146\n",
      "Epoch 8/10\n",
      "29068/29068 [==============================] - 182s 6ms/step - loss: 0.2195 - acc: 0.9218 - val_loss: 1.8037 - val_acc: 0.5892\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.61146\n",
      "Epoch 9/10\n",
      "29068/29068 [==============================] - 182s 6ms/step - loss: 0.1823 - acc: 0.9366 - val_loss: 1.8380 - val_acc: 0.5957\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.61146\n",
      "Epoch 10/10\n",
      "29068/29068 [==============================] - 182s 6ms/step - loss: 0.1574 - acc: 0.9457 - val_loss: 1.8319 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.61146 to 0.61455, saving model to weights-improvement-model2-10-0.61.hdf5\n"
     ]
    }
   ],
   "source": [
    "model2.fit(train_bigger_inv, train_bigger_labels, \n",
    "          batch_size=128, validation_split=0.1, epochs=10, verbose=1, callbacks=callback_list)\n",
    "model2.save_weights(\"best_weights_model2.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = model2.evaluate(priv_test_ds_inv, priv_test_labels, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.7575682035957947, 0.611869601568627]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"floatx\": \"float32\",\r\n",
      "    \"epsilon\": 1e-07,\r\n",
      "    \"backend\": \"tensorflow\",\r\n",
      "    \"image_data_format\": \"channels_last\"\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat ~/.keras/keras.json"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
